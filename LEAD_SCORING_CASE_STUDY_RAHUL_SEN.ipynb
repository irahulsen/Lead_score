# LEAD_SCORING_CASE_STUDY
## Leads Conversion Using Logistic Regression


#### You have been provided with a leads dataset from the past with around 9000 data points. This dataset consists of various attributes such as Lead Source, Total Time Spent on Website, Total Visits, Last Activity, etc. which may or may not be useful in ultimately deciding whether a lead will be converted or not. The target variable, in this case, is the column ‘Converted’ which tells whether a past lead was converted or not wherein 1 means it was converted and 0 means it wasn’t converted. You can learn more about the dataset from the data dictionary provided in the zip folder at the end of the page. Another thing that you also need to check out are the levels present in the categorical variables. Many of the categorical variables have a level called 'Select' which needs to be handled because it is as good as a null value (think why?).

## Goals of the Case Study
### There are quite a few goals for this case study:

- Build a logistic regression model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.
- There are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. These problems are provided in a separate doc file. Please fill it based on the logistic regression model you got in the first step. Also, make sure you include this in your final PPT where you'll make recommendations.

## Results Expected
1. A well-commented Jupyter notebook with at least the logistic regression model, the conversion predictions and evaluation metrics.
2. The word document filled with solutions to all the problems.
3. The overall approach of the analysis in a presentation.
    - Mention the problem statement and the analysis approach briefly 
    - Explain the results in business terms
    - Include visualisations and summarise the most important results in the presentation
4. A brief summary report in 500 words explaining how you proceeded with the assignment and the learnings that you gathered.

#Installing Package
!pip install plotly

# Steps to Perform.
0. Import Packages.
1. Reading Dataset.
2. Basic Information.
3. Handling Missing Values
4. EDA : Exploratory Data Analysis
5. Outlier Detection and Capping.
6. Features Selection Based on correlation and Select K method.
7. Model Selection and Training. 



## Step 1: Import Packages


#supressing warnings
import warnings
warnings.filterwarnings("ignore")

## Step 0: Import packages
### Step 0.1 Plackages for basic data manuplation.
import pandas as pd ## Pandas packages for data manuplation and using DataFrames.
import numpy as np ## For some mathmetical or matrics manuplation.

### Step 0.2: Ploting Libraries
import matplotlib.pyplot as plt ## For EDA or ploting Graphs
import seaborn as sns ## For EDA or ploting advance graphs.
import plotly.express as px ## For EDA or advance plots.

# setting the max row & columns display to 130
pd.set_option("display.max_columns",130)
pd.set_option("display.max_rows",130)

## Step 1. Reading Dataset

## Step 1. Reading Datasets in dataframe
PATH  = "./Leads.csv"
df = pd.read_csv(PATH)
df.head()

## Observations
1. As we can see that is a defines as a uniquely identfier of each row. We will drop this as it will not be much usefull for getting inferences and train better quality ML model. 

## Step 2. Basic Data Manuplation.

## Step 2.1 Shape of dataset.
num_rows = f"No. of rows: {df.shape[0]}"
num_cols = f"No. of cols: {df.shape[1]}"
dimentation = f"{num_rows} x {num_cols}"
print("<-----------------------Shape of dataset--------------------------->")
print(dimentation)

## Step 2.2 Data type info about the datasets.
df.info()   ## This method prints the dtypes info and the no of cols and row in datasets.

## Observations
1. Here 30 columns are Object these are the majority column data types. Also 4 float64 and 3 int64 columns are also there.
2. Also Our Target Column "Converted" is a int64 dtype. 

## Step 2.3 Num of null values in each col.
df.isnull().sum()

## Observations
1. Here some columns has more then half entries as Null or None, in the cleaning process we will discard all of these columns.
2. We will include all of these columns that has null value percentage less then 5%.
3. Also in the problem statement some of the columns has categorical value name "Select", This value is here because the form that users filed has select with not required so, where users did not feel any data point that data point will marks "Select"
4. First we will count this value in all categorical columns and select the columns based on decided ratio.

# Step : 2.4 Ploting heat map for null values for application data.
sns.heatmap(df.isnull(),yticklabels = False, cbar= False, cmap = "viridis").set_title("No. of null values in data");

## Observations
1. Here Yellow lines show the missing values higher the yello lines, higher the missing values. 

#Step 2.5 Checking for Nan values percentage on application dataset
def check_percentage(df, cols  =  None, thresh =  0):
    """
    returns the missing val column list and print the percentage missing values in each column.
    Args:
        df(pandas.dataframe): Pandas dataframe. 
    Returns:
        missing_val_col(List): List of columns names that has missing values.
    """
    missing_val_col,count,sum_null = [],0,df.isna().sum()
    if(cols == None):
        cols = df.columns
    for col in cols:
        pct = df[col].isna().mean() * 100
        if(sum_null[col]!=0 and pct > thresh):
            count+=1
            missing_val_col.append(col)
            print(f"{col} : {sum_null[col]} => {round(pct, 2)}%")
    print()
    print("No. Of Columns: ", count)
    return missing_val_col

# function calling    
drop_columns = check_percentage(df,thresh=5)

## Observations
1. Here 13 columns has higher missing value from thresh hold of 5. Need to drop these columns.
2. Also we will drop the  Prospect ID column as we discussed that is no use in further analysis

## Step 3 Handling Missing Values

### Step 3.1 Droping Missing Values

# Step 3.1.1 Dropping those columns that has more then 5% missing values.
drop_columns.append("Prospect ID")  ## Including the prospect id.
df.drop(drop_columns, axis=1, inplace= True)

#Step: 3.1.2 Shape of Data Frame.
## Step 2.1 Shape of dataset.
num_rows = f"No. of rows: {df.shape[0]}"
num_cols = f"No. of cols: {df.shape[1]}"
dimentation = f"{num_rows} x {num_cols}"
print("<-----------------------Shape of dataset After Droping Null Values columns--------------------------->")
print(dimentation)

# Step 3.2.1 replacing nan values to a cat features nan values.
cat_feature_nan = [col for col in df.columns if df[col].dtype == 'O']
cat_feature_nan
def replacing_nan_val(dataset, feature_nan):
    data= dataset.copy()
    data[feature_nan] = data[feature_nan].fillna("Empty")
    return data


## Calling the features.
missing_value = check_percentage(df,cat_feature_nan)
df = replacing_nan_val(df,missing_value)

## Observations
1. Here only two categorical columns that has missing values. 
2. We replaced Missing values to a new category Empty.

# Step 3.2.2 Replaceing nan values to numerical values.
numerical_features_nan = [col for col in df.columns if df[col].dtype != 'O']

def replace_value(dataset, feature_nan):
    data= dataset.copy()
    data[feature_nan] = data[feature_nan].fillna(data[feature_nan].mean())
    return data

# Calling function.
numerical_value_nan = check_percentage(df, numerical_features_nan)
df =  replace_value(df,numerical_value_nan)

## Observations
1. Here two columns has missing values so we replaced with mode value of each columns.

df.info()

# Step 3.3 Checking the null values.
sns.heatmap(df.isnull(),yticklabels = False, cbar= False, cmap = "viridis").set_title("No. of null values in data");

## Inference:
1. All the null null imputed. for categorical col we replace the values with new category and for numerical col we replaced with mean value.

## Step 4. EDA: Exploratry Data Analysis

## Step 4.1 Basic Descerete Statistic Analysis on Application dataset.
df.describe()

## Observations
1. Here 5 columns are Numerical columns. 
2. Out of 5 four columns has minimum value zero.
3. Converted column that is our target column  has 50% Zero values that means there much more percentage of people that not converted. 

## Our Target column is Converted
TARGET_COLUMN = "Converted"


## Extracting the categorical columns.
CAT_COLS = [col for col in df if df[col].dtype =='O']

## Extracting the numerical columns.
NUM_COLS  = [col for col in df if df[col].dtype != 'O']

## No. of categorical columns and numerical columns.
print(f"No. of categorical columns : {len(CAT_COLS)}")
print(f"No. of numerical columns   : {len(NUM_COLS)}")

## Step 4.2 Plotting the target column distribution.
def plot_target_distribution(dataframe, target_col_name):
    """
    Plot a count plot of the distribution of the target column.

    Parameters:
        dataframe (pandas.DataFrame): DataFrame containing the data.
        target_col_name (str): Name of the target column in the DataFrame.

    Returns:
        None
    """
    target_col = dataframe[target_col_name]
    fig = px.histogram(target_col,
                       labels={'value': target_col_name},
                       title=f"Distribution of {target_col_name}",
                       marginal='violin',
                       color_discrete_sequence=['green'])
    fig.update_layout(
        xaxis_title_text=f'{target_col_name}',
        yaxis_title_text='Count',
        bargap=0.2,
        showlegend=False
    )
    fig.show()

## Ploting count plot of target variable to see the distribution of two target category
plot_target_distribution(df,TARGET_COLUMN)

## Observation
1. As we earlier predicted that Target column has higher number of 0 values then 1. We have more negative examples in our training then positive. 

## Step 4.3 Bivariate Analysis with Categorical columns.
# Step 4.3.1 No. of classes in categorical columns.
df[CAT_COLS].nunique()

## Observations
1. Here most of the categorical columns has 2 categories.
2. Lead Source that seems to be an sources from where we get the leads of customers is 21 that is highest categories among all the columns.

# Step 4.4` we will be using transformation
for feature in CAT_COLS:
    if(0 in df[feature].unique()):
        pass
    else:
        #trying to log normal transformation
        sns.countplot(x=feature, hue=TARGET_COLUMN, data=df)
        plt.title(f"{feature} Vs TARGET")
        plt.xticks(rotation=90)  # Rotate x-labels by 90 degrees
        plt.show()


## Observations
1. Here some categories has higher number of value then others in a single column. 
2. higher number categories supports to not  conversion and less number of categories suuport conversion. 
3. Like in Last Notable Activity if we sent the SMS then chance of convertion is higher. 
4. Also if a source is references then there is higher chance of conversion of that person. 
5. Olark chat conversion has higher negetive impact on the conversion. So, we need the inchance the bot response.

for feature in CAT_COLS:
    gp_df = df.groupby(feature).mean(TARGET_COLUMN)
    #trying to log normal transformation
    sns.set(style="whitegrid")  # Optional: Set the style
    plt.figure(figsize=(8, 8))
    sns.set_palette("pastel")  # Optional: Set the color palette

    # Plot the pie chart
    plt.pie(gp_df[TARGET_COLUMN].values, labels=gp_df[TARGET_COLUMN].index, autopct='%1f%%', startangle=140)

    # Set the aspect ratio to be equal for a circular pie chart
    plt.axis('equal')

    # Show the plot
    plt.title(f'{feature} Vs {TARGET_COLUMN}')
    plt.show()

## Observation
1. As we can see that some the categorical columns contribute 100% to not conversion. 
2. Also some of the cat cols like Do not email. Do not call has higher number of negetive corelation to conversion. 
3. Also references or new paper articles are the effective sources till now.

## Step 4.5 Bivariate on Numerical columns

DESCERETE_NUM_COL = [col for col in NUM_COLS if len(df[col].unique()) < 25]
CONTINUOUS_NUM_COL = [col for col in NUM_COLS if col not in DESCERETE_NUM_COL]
print(f"Descerete Numerical Cols :{len(DESCERETE_NUM_COL)} : Continuous Numerical Cols : {len(CONTINUOUS_NUM_COL)}")


## Observations
1. Here number of column is No descerete columns are 1 and continuous cols are 15 

## Step :4.6  Univariate analysis of continuous numerical columns.
# Create a histogram plot for a specific column
for feature in CONTINUOUS_NUM_COL:
    sns.histplot(data=df, x=feature, bins=10, kde=False, color='green')
    # Set plot labels and title
    plt.xlabel('Values')
    plt.ylabel('Frequency')
    plt.title('Histogram Plot')
    plt.xticks(rotation=90)  # Rotate x-labels by 90 degrees
    # Show the plot
    plt.show()

## Observations:
1. Distributionof these continuous numerical columns are not normalized these are left skewed. Also Some of then have outliers because of these we can see the barr at one side and one single value on other side.

## Bivariate analysis of descerete variable.
for feature in CONTINUOUS_NUM_COL:
    # Create a bar plot
    sns.barplot(x=TARGET_COLUMN, y=feature, data=df)

    # Set plot labels and title
    plt.xlabel(f'{TARGET_COLUMN}')
    plt.ylabel(f'{feature}')
    plt.title(f'Bar Plot of {feature} vs {TARGET_COLUMN}')
    # Show the plot
    plt.show()

## Observations
1. Here we can see that time spent and TotalVisit on website has higher potitive impact on conversion
2. And page views has netural impact on the conversion.

## Step 5: Outlier detection 

## Step 5.1: Checking is their any outliers or not.
## Step 5.1.1: Box plot for outlier detection.
for feature in CONTINUOUS_NUM_COL:
    # Create a box plot
    sns.boxplot(x=df[feature])

    # Set plot labels and title
    plt.xlabel(f'{feature}')
    plt.title('Box Plot for Outlier Detection')

    # Show the plot
    plt.show()

## Observations
1. In total visits and page views per visit has outliers. 
2. Removing outliers does not solve the problem we need to cap the outlier that means if some day total visit per page goes higher then a threshold then we will cap this with thresh value for better results from the model.

## Step 5.2.1: using z-index method where  we use iqr to detect the outler then we cap the outlier value.
def handle_outlier(df,features):
    for feature in features:
        percentile25 = df[feature].quantile(0.25)
        percentile75 = df[feature].quantile(0.75)
        iqr  = percentile75 - percentile25
        upper_limit = percentile75 + (1.5)*iqr
        lower_limit = percentile25 - (1.5)*iqr
        if(upper_limit >0):
            # df = df[df[feature]< upper_limit]
            # df = df[df[feature]>lower_limit]
            df[feature] = df[feature].apply(lambda x : x if x < upper_limit else upper_limit)
            df[feature] = df[feature].apply(lambda x : x if x> lower_limit else lower_limit)
    return df

df = handle_outlier(df,CONTINUOUS_NUM_COL)

## Step 5.2.2 checking the after effect of outlier capping.
for feature in CONTINUOUS_NUM_COL:
    # Create a box plot
    sns.boxplot(x=df[feature])

    # Set plot labels and title
    plt.xlabel(f'{feature}')
    plt.title('Box Plot for Outlier Detection')

    # Show the plot
    plt.show()

## Observations
1. Now there is not outliers, we cap the outliers using z-index method. where we cap the outlier between IQR range. 


## Step 4.7 Multivariate Analysis
sns.pairplot(df[CONTINUOUS_NUM_COL])

# Show the plot
plt.show()

df[CAT_COLS].shape

## Ovservations:
1. In the pair plot we can see the scatter plots between multiple independent numerical variables that follows some trend or has some correlation between them.


df.head()

# List of variables to map

varlist =  ['Do Not Email','Do Not Call', 'Search', 'Magazine', 'Newspaper Article', 'X Education Forums',
       'Newspaper', 'Digital Advertisement', 'Through Recommendations',
       'Receive More Updates About Our Courses',
       'Update me on Supply Chain Content', 'Get updates on DM Content',
       'I agree to pay the amount through cheque',
       'A free copy of Mastering The Interview']

# Defining the map function
def binary_map(x):
    return x.map({'Yes': 1, "No": 0})

# Applying the function to the housing list
df[varlist] = df[varlist].apply(binary_map)

df.head()

varlist2 = df.columns[df.dtypes=="object"]
varlist2

# Creating a dummy variable for some of the categorical variables and dropping the first one.
dummy1 = pd.get_dummies(df[varlist2], drop_first=True)
# Adding the results to the master dataframe
leads_df = pd.concat([df, dummy1], axis=1)

leads_df.shape

# Dropping the repeated variables
leads_df= leads_df.drop(varlist2,1)
leads_df.shape

leads_df.dtypes

## Test-Train Split

# Import the required library
from sklearn.model_selection import train_test_split
# Putting feature variable to X
X = leads_df.drop(["Lead Number","Converted"],axis= 1)
# Putting response variable to y
y = leads_df['Converted']
X_train, X_test, y_train,y_test = train_test_split (X,y,train_size=0.7,test_size=0.3, random_state=100)

# Step 5: Feature Scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()


X_train[["TotalVisits","Total Time Spent on Website","Page Views Per Visit"]] = scaler.fit_transform(X_train[["TotalVisits","Total Time Spent on Website","Page Views Per Visit"]])

X_train.head()

# Model Building

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()

from sklearn.feature_selection import RFE
rfe = RFE(logreg, n_features_to_select=15)              # running RFE with 15 variables as output
rfe = rfe.fit(X_train, y_train)

rfe.support_

list(zip(X_train.columns, rfe.support_, rfe.ranking_))

col = X_train.columns[rfe.support_]

X_train = X_train[col] # selecting columns from RFE selected 15 features

# Assessing the model with StatsModels

import statsmodels.api as sm

# Model - 1

X_train_sm = sm.add_constant(X_train)
logm = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm.fit()
res.summary()

# The p-value of some variables are more than 0.05.
# We will check vif and then proceed for variable elimination.

# Check for the VIF values of the feature variables. 
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train.columns
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

# VIF score is also high. First we will remove p values. then will check for VIF.
# Let's remove "Last Activity_Email Received" then re-built the model again.

# Model - 2

X_train.drop('Last Activity_Email Received', axis = 1, inplace = True)

X_train_sm = sm.add_constant(X_train)
logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm2.fit()
res.summary()

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train.columns
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

# p values are still not OK.
# Let's remove "Lead Source_Reference" then re-built the model again.

# Model - 3

X_train.drop('Lead Source_Reference', axis = 1, inplace = True)

X_train_sm = sm.add_constant(X_train)
logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm3.fit()
res.summary()

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train.columns
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

# The p values are still not OK. # Let's remove "Last Notable Activity_Had a Phone Conversation" then re-built the model again.

# Model - 4

X_train.drop("Last Notable Activity_Had a Phone Conversation", axis = 1, inplace = True)

X_train_sm = sm.add_constant(X_train)
logm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm4.fit()
res.summary()

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train.columns
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

# now the p-values are fine and but VIF is above 5.
# Let's remove "Last Activity_Email Opened" then re-built the model again.

# Model - 5

X_train.drop("Last Activity_Email Opened", axis = 1, inplace = True)

X_train_sm = sm.add_constant(X_train)
logm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm5.fit()
res.summary()

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train.columns
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

# now the VIFs are fine and but p value is above 0.05.
# Let's remove "Last Activity_Email Opened" then re-built the model again.

X_train.drop("Last Notable Activity_Email Opened", axis = 1, inplace = True)

# Model - 6

X_train_sm = sm.add_constant(X_train)
logm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())
res = logm6.fit()
res.summary()

# Create a dataframe that will contain the names of all the feature variables and their respective VIFs
vif = pd.DataFrame()
vif['Features'] = X_train.columns
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

# both VIF and p values are fine. We will proceed with model 6

coefficient = res.params

coefficient.sort_values(ascending=False)

X_train.shape

# Getting the predicted values on the train set
y_train_pred = res.predict(X_train_sm)
y_train_pred[:10]

y_train_pred = y_train_pred.values.reshape(-1)
y_train_pred[:10]

# Creating a dataframe with the actual convertion and the predicted probabilities

y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Pred_prob':y_train_pred})
y_train_pred_final.head()

# Creating new column 'predicted' with 1 if Pred_prob > 0.5 else 0

y_train_pred_final['predicted'] = y_train_pred_final.Pred_prob.map(lambda x: 1 if x > 0.5 else 0)

# Let's see the head
y_train_pred_final.head()

from sklearn import metrics

# Confusion matrix 
confusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )
print(confusion)

# Predicted     Cold_lead    Hot_lead
# Actual
# Cold_lead         3508      494
# Hot_lead          807       1659  

TP = confusion[1,1] # true positive 
TN = confusion[0,0] # true negatives
FP = confusion[0,1] # false positives
FN = confusion[1,0] # false negatives

# Let's check the overall accuracy.
print(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))

# 79.88 % accuracy. It's preety good score.

### Step 9: Plotting the ROC Curve
### An ROC curve demonstrates several things:

- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).
- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.
- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.

def draw_roc( actual, probs ):
    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,
                                              drop_intermediate = False )
    auc_score = metrics.roc_auc_score( actual, probs )
    plt.figure(figsize=(5, 5))
    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

    return None

fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Pred_prob, drop_intermediate = False )

# Call the ROC function
draw_roc(y_train_pred_final.Converted, y_train_pred_final.Pred_prob)

## Now we have to decide the Optimal Cutoff Point based upon Sensitivity- Specificity OR Recall- Precision

## As per the business need if the model predicts " a cold lead to be a hot lead" , the marketing team will boost their effort
## to convert it. But the whole effort (cost) will go in vain.
## but if the model prdecits " an acutal hot lead a cold lead" , It will not have any effect in the cost of the company as
##  the hot lead anyway going to convert to  revenue.

## as per the above discusion we can conclude that False positive (" a cold lead to be a hot lead")
## is more important than the False negative (" an acutal hot lead a cold lead") ones.
## If we reduce the Wrong 445 hot lead prediction, we will overcome the business problem. So it is a Recall- Precision problem.

## Precision and Recall

from sklearn.metrics import precision_score, recall_score

print("Precision_Score = ",precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted))
print("recall_score = ", recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))

from sklearn.metrics import precision_recall_curve

y_train_pred_final.Converted, y_train_pred_final.predicted

p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Pred_prob)

plt.plot(thresholds, p[:-1], "g-")
plt.plot(thresholds, r[:-1], "r-")
plt.show()

# As per the above graph we can see the optimal value for cut-off is near around 0.42.

y_train_pred_final['final_predicted'] = y_train_pred_final.Pred_prob.map(lambda x: 1 if x > 0.42 else 0)
y_train_pred_final.head()

#Lead_Score = "Pred_prob" * 100
y_train_pred_final['Lead_score'] = y_train_pred_final["Pred_prob"]*100

#Lead_type :Hot lead OR Cold_lead based upon the cutoff lead score of 42
y_train_pred_final['Lead_type'] = y_train_pred_final["Lead_score"].map(lambda x: "Hot_lead" if x > 42 else "Cold_lead")

y_train_pred_final

# Accuracy
print("Train_Accuracy = ",round(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)*100,2))

# Creating confusion matrix again
confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )
confusion2

# Predicted     Cold_lead    Hot_lead
# Actual
# Cold_lead         3359      643
# Hot_lead          660       1806  

TP = confusion2[1,1] # true positive 
TN = confusion2[0,0] # true negatives
FP = confusion2[0,1] # false positives
FN = confusion2[1,0] # false negatives

print("Precision_Score = ",precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted))
print("recall_score = ", recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))

# Precision = TP / TP + FP
TP / (TP + FP)

#Recall = TP / TP + FN
TP / (TP + FN)

# Making predictions on the test set

X_test[["TotalVisits","Total Time Spent on Website","Page Views Per Visit"]] = scaler.fit_transform(X_test[["TotalVisits","Total Time Spent on Website","Page Views Per Visit"]])

X_test.head()

X_test.shape

X_test = X_test[col]
X_test = X_test.drop(['Last Activity_Email Received','Lead Source_Reference',
                      "Last Notable Activity_Had a Phone Conversation","Last Activity_Email Opened",
                      "Last Notable Activity_Email Opened"],axis=1)
X_test.shape

X_test_sm = sm.add_constant(X_test)

y_test_pred = res.predict(X_test_sm)

y_test_pred[:10]

# Converting y_pred to a dataframe which is an array
y_pred_1 = pd.DataFrame(y_test_pred)

# Converting y_test to dataframe
y_test_df = pd.DataFrame(y_test)

# Removing index for both dataframes to append them side by side 
y_pred_1.reset_index(drop=True, inplace=True)
y_test_df.reset_index(drop=True, inplace=True)

# Appending y_test_df and y_pred_1
y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)

# Renaming the column 
y_pred_final= y_pred_final.rename(columns={ 0 : 'Pred_prob'})

y_pred_final.head()

# Making prediction using cut off 0.42
y_pred_final["final_predicted"] = y_pred_final.Pred_prob.map(lambda x: 1 if x > 0.42 else 0)
y_pred_final.head()

#Lead_Score = "Pred_prob" * 100
y_pred_final['Lead_score'] = y_pred_final["Pred_prob"]*100
y_pred_final

#Lead_type :Hot lead OR Cold_lead based upon the cutoff lead score of 42
y_pred_final['Lead_type'] = y_pred_final["Lead_score"].map(lambda x: "Hot_lead" if x > 42 else "Cold_lead")

y_pred_final

# Check the overall accuracy
metrics.accuracy_score(y_pred_final['Converted'], y_pred_final.final_predicted)

# Creating confusion matrix 
confusion3 = metrics.confusion_matrix(y_pred_final['Converted'], y_pred_final.final_predicted )
confusion3

TP = confusion3[1,1] # true positive 
TN = confusion3[0,0] # true negatives
FP = confusion3[0,1] # false positives
FN = confusion3[1,0] # false negatives

# Precision = TP / TP + FP
print(TP / (TP + FP))

#Recall = TP / TP + FN
print(TP / (TP + FN))

print("Precision_Score = ",precision_score(y_train_pred_final.Converted, y_train_pred_final.predicted))
print("recall_score = ", recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted))